{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Songs Using an LSTM from Genius Lyrics\n",
    "Data collection info here ---> https://github.com/johnwmillr/LyricsGenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from LSTM import *\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "torch.manual_seed(42069)\n",
    "np.random.seed(42069)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader('Lyrics_ChiefKeef.json', with_tags=True)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "Title: Love Sosa\n",
      "[Spoken Intro: Jordan Gilty]\n",
      "Fuckers in school telling me, always in the barber shop\n",
      "\"Chief Keef ain't 'bout this, Chief Keef ain't 'bout that\n",
      "My boy a BD on fucking Lamron and them\n",
      "He, he, they say that nigga don't be putting in no work”\n",
      "Shut the fuck up!\n",
      "Y'all niggas ain’t know shit!\n",
      "All ya motherfuckers talk about...\n",
      "“Chief Keef ain't no hitta, Chief Keef ain't this, Chief Keef a fake”\n",
      "Shut the fuck up!\n",
      "Y'all don't real with that nigga!\n",
      "Y'all know that nigga got caught with a ratchet\n",
      "Shootin' at the police and shit\n",
      "Nigga been on probation since, fuckin', I don't know when!\n",
      "Motherfucka, stop fuckin' playin' him like that\n",
      "Them niggas savages out there!\n",
      "If I catch another motherfucker talking sweet about Chief Keef\n",
      "I'm fucking beating they ass!\n",
      "I'm not fucking playing no more!\n",
      "Y'know them niggas roll with Lil Reese and them\n",
      "(Young Chop on the beat)\n",
      "\n",
      "[Chorus]\n",
      "These bitches love Sosa\n",
      "O End or no end\n",
      "Fuckin' with them O boys\n",
      "You gon' get fucked over\n",
      "Raris and Rovers\n",
      "These hoes love Chief Sosa\n",
      "Hit him with that cobra\n",
      "Now that boy slumped over\n",
      "They do it all for Sosa\n",
      "You boys ain't making no noise\n",
      "Y'all know I'm a grown boy\n",
      "Your clique full of broke boys\n",
      "God y'all some broke boys\n",
      "God y'all some broke boys\n",
      "We GBE dope boys\n",
      "We got lots of dough boy\n",
      "\n",
      "[Verse 1]\n",
      "These bitches love Sosa\n",
      "And they love them Glo' Boys\n",
      "Know we from the 'Go boy\n",
      "But we cannot go boy\n",
      "No I don't know ol' boy\n",
      "I know he's a broke boy\n",
      "Raris and Rovers\n",
      "Convertible Lambos boy\n",
      "You know I got bands boy\n",
      "And it's in my pants boy\n",
      "Disrespect them O Boys\n",
      "You won't speak again boy\n",
      "Don't think that I'm playin' boy\n",
      "No we don't use hands boy\n",
      "No we don't do friends boy\n",
      "Collect bands I'm a landlord\n",
      "I gets lotsa commas\n",
      "I can fuck yo mama\n",
      "I ain't with the drama\n",
      "You can meet my llama\n",
      "Ridin' with 3hunna\n",
      "With 300 foreigns\n",
      "These bitches see Chief Sosa\n",
      "I swear to god, they all honored\n",
      "\n",
      "[Chorus]\n",
      "These bitches love Sosa\n",
      "O End or no end\n",
      "Fuckin' with them O boys\n",
      "You gon' get fucked over\n",
      "Raris and Rovers\n",
      "These hoes love Chief Sosa\n",
      "Hit him with that cobra\n",
      "Now that boy slumped over\n",
      "They do it all for Sosa\n",
      "You boys ain't making no noise\n",
      "Y'all know I'm a grown boy\n",
      "Your clique full of broke boys\n",
      "God y'all some broke boys\n",
      "God y'all some broke boys\n",
      "We GBE dope boys\n",
      "We got lots of dough, boy\n",
      "\n",
      "[Verse 2]\n",
      "Don't make me call D. Rose boy\n",
      "He six double-O boy\n",
      "And he keep that pole boy\n",
      "You gon' get fucked over\n",
      "Bitch, I done sell soda\n",
      "And I done sell coca\n",
      "She gon' clap for Sosa\n",
      "He gon' clap for Sosa\n",
      "They do it for Sosa\n",
      "Them hoes, they do it for Sosa\n",
      "Tadoe off the molly water\n",
      "So nigga be cool like water\n",
      "'Fore you get hit with this lava\n",
      "Bitch I'm the trending topic\n",
      "Don't care no price, I'll cop it, B\n",
      "And yo bitch steady jockin' me\n",
      "\n",
      "[Chorus]\n",
      "These bitches love Sosa\n",
      "O End or no end\n",
      "Fuckin' with them O boys\n",
      "You gon' get fucked over\n",
      "Raris and Rovers\n",
      "These hoes love Chief Sosa\n",
      "Hit him with that cobra\n",
      "Now that boy slumped over\n",
      "They do it all for Sosa\n",
      "You boys ain't making no noise\n",
      "Y'all know I'm a grown boy\n",
      "Your clique full of broke boys\n",
      "God y'all some broke boys\n",
      "God y'all some broke boys\n",
      "We GBE dope boys\n",
      "We got lots of dough boy\n",
      "<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.songs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(42069)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nnet(\n",
       "  (lstm_layer): LSTM(109, 300, num_layers=3, dropout=0.25)\n",
       "  (fc): Linear(in_features=300, out_features=109, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(dataset.char_dict)\n",
    "hidden_dim = 300\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "\n",
    "model = Nnet(input_dim, hidden_dim, n_layers, dropout=0.25)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss criteria are defined in the torch.nn package\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 1/200... Training Loss: 3.002227... Validation Loss: 3.001346\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (inf --> 3.001346).  Saving model ...\n",
      "Epoch: 2/200... Training Loss: 2.549742... Validation Loss: 2.555988\n",
      "Validation loss decreased (3.001346 --> 2.555988).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "clip = 10\n",
    "chunk_len = 300\n",
    "early_stop = True\n",
    "early_stop_thresh = 15\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "val_losses_epoch = []\n",
    "train_losses_epoch = []\n",
    "\n",
    "def get_labels(inputs):\n",
    "    labels = torch.zeros(inputs.shape[0])\n",
    "\n",
    "    for i,oneHot in enumerate(inputs[1:]):\n",
    "        idx = oneHot.argmax().item()\n",
    "        labels[i] = idx\n",
    "        \n",
    "    return labels\n",
    "\n",
    "\n",
    "index_to_char = {v: k for k, v in dataset.char_dict.items()}\n",
    "\n",
    "model.train()\n",
    "print(\"Training...\")\n",
    "for epoch in range(epochs):\n",
    "    # training process\n",
    "    for sample_idx, song in enumerate(train_loader):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs, h)\n",
    "            loss = criterion(output, labels)\n",
    "            h = tuple([e.detach() for e in h])\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "    # validation process\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    #print(\"Evaluating on validation set...\")\n",
    "    for sample_idx, song in enumerate(val_loader):\n",
    "#         if sample_idx % 100 == 0:\n",
    "#             print(\"Evaluating on song %d (%d/%d)\"  % (sample_idx, sample_idx, len(val_loader)))\n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        \n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "    # evaluate on the training set to get training loss metrics\n",
    "    train_losses = []\n",
    "    #print(\"Evaluting on training set...\")\n",
    "    for sample_idx, song in enumerate(train_loader):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            output, h = model(inputs, h)\n",
    "            train_loss = criterion(output, labels)\n",
    "            train_losses.append(train_loss.item())\n",
    "    \n",
    "    \n",
    "    train_losses_epoch.append(np.mean(train_losses))\n",
    "    val_losses_epoch.append(np.mean(val_losses))\n",
    "\n",
    "    model.train()\n",
    "    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "          \"Training Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "          \"Validation Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Creating training checkpoint...')\n",
    "        # save training progress every five epochs\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, 'chief_keef_state_ckpt.pth')\n",
    "    \n",
    "    if val_losses_epoch[epoch] <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './chief_keef_state_dict.pth')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "        valid_loss_min = np.mean(val_losses)\n",
    "        \n",
    "        if early_stop:\n",
    "            early_stop_strikes = 0\n",
    "    \n",
    "    else:\n",
    "        if early_stop:\n",
    "            early_stop_strikes += 1\n",
    "            if early_stop_strikes >= early_stop_thresh:\n",
    "                print(\"Early stopping is enabled. Training will now stop.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Early stopping is enabled. Training will stop in %d epochs if validation loss does not go down.\"\n",
    "                    % (early_stop_thresh-early_stop_strikes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(train_losses_epoch)\n",
    "plt.plot(val_losses_epoch)\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss (averaged over each chunk)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from torch.distributions.categorical import Categorical\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "model = Nnet(input_dim, hidden_dim, n_layers, dropout=0.25)\n",
    "model.load_state_dict(torch.load('chief_keef_state_dict.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_next_char(model, char_dict, hidden, input_token='<start>', sampling_scheme='softmax', temperature=0.7):\n",
    "    '''Gets the next character given a chunk input_token\n",
    "    :param: a pytorch neural network to be passed\n",
    "    :char_dict: a dictionary mapping alphanumeric characters of type str to integers\n",
    "    :input_token: the string to begin on for the LSTM\n",
    "    '''\n",
    "    inputs = encode_chunk(input_token, char_dict)\n",
    "    h = tuple([e.data for e in hidden])\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    if sampling_scheme == 'softmax':\n",
    "        prediction_vector = softmax(output / temperature, dim=1)\n",
    "        output_dist = Categorical(probs=prediction_vector)\n",
    "        next_char = output_dist.sample()[-1]\n",
    "    else:\n",
    "        # argmax sampling scheme\n",
    "        prediction_vector = softmax(output, dim=1)\n",
    "        next_char = prediction_vector.argmax(dim=1)[-1]\n",
    "    \n",
    "    return next_char, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation progress: 0/3000 characters\n",
      "Generation progress: 500/3000 characters\n",
      "Generation progress: 1000/3000 characters\n",
      "Generation progress: 1500/3000 characters\n",
      "Generation progress: 2000/3000 characters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e7f3798c0051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_chunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_chunk_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generated_file_size = 3000 # characters\n",
    "model_name = 'ChiefKeef_LSTM'\n",
    "\n",
    "next_chunk = '<start>\\nTitle:'\n",
    "\n",
    "out = deepcopy(next_chunk)\n",
    "    \n",
    "index_to_char = {v: k for k, v in dataset.char_dict.items()}\n",
    "\n",
    "max_chunk_size = 500\n",
    "#try passing in the token with 0'd hidden units at the beginning then take the hidden unit output\n",
    "#from that and pass that back in with the token again and go on from there\n",
    "hidden = model.init_hidden(1)\n",
    "for progress,i in enumerate(range(generated_file_size)):\n",
    "    if progress % 500 == 0:\n",
    "        print(\"Generation progress: %d/%d characters\" % (progress, generated_file_size))\n",
    "    if progress == 0 or next_chunk[-len('<end>'):] == '<end>':\n",
    "        if progress > 0:\n",
    "            next_chunk = '<start>\\n'\n",
    "            out += '\\n<start>\\n'\n",
    "        #hidden = model.init_hidden(1)\n",
    "    \n",
    "    (next_char, h) = get_next_char(model, dataset.char_dict, hidden, input_token=next_chunk)\n",
    "    char = index_to_char[next_char.item()]\n",
    "    \n",
    "    if len(next_chunk) < max_chunk_size:\n",
    "        next_chunk += char\n",
    "    else:\n",
    "        next_chunk = next_chunk[1:] + char\n",
    "    out += char\n",
    "    \n",
    "with open(\"generated_songs/generated_songs_\" + model_name + \".txt\", \"w\") as text_file:\n",
    "    print(out, file=text_file)\n",
    "    \n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
