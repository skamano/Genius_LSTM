{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Songs Using an LSTM from Genius Lyrics\n",
    "Data collection info here ---> https://github.com/johnwmillr/LyricsGenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from LSTM import *\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "torch.manual_seed(42069)\n",
    "np.random.seed(42069)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader('Lyrics_KanyeWest.json', with_tags=True)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "Title: Mercy\n",
      "[Intro: Fuzzy Jones]\n",
      "Well, it is a weepin' and a moanin' and a gnashin' of teeth\n",
      "It is a weepin' and a mournin' and a gnashin' of teeth\n",
      "It is a—when it comes to my sound which is the champion sound\n",
      "Believe, believe\n",
      "\n",
      "[Chorus: YB, Big Sean & Fuzzy Jones]\n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-I'm in that two-seat Lambo\n",
      "With your girl, she tryna jerk me (Believe)\n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty\n",
      "I-I-I-I-I'm in that two-seat Lambo\n",
      "With your girl, she tryna jerk me\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo\n",
      "With your girl, she tryna jerk me (Woah, believe)\n",
      "O-o-o-o-o-okay, Lamborghini Mercy\n",
      "Your chick, she so thirsty (Boy)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Boy)\n",
      "With your girl, she tryna jerk me\n",
      "\n",
      "[Verse 1: Big Sean & Kanye West]\n",
      "Okay, drop it to the floor, make that ass shake (Shake, shake)\n",
      "Woah, make the ground move: that's an ass quake\n",
      "Built a house up on that ass: that's an ass-state\n",
      "Roll–roll–roll my weed on it: that's an ass tray\n",
      "Say, Ye, say, Ye, don't we do this every day–day? (Huh?)\n",
      "I work them long nights, long nights to get a payday (Huh?)\n",
      "Finally got paid, now I need shade and a vacay\n",
      "And niggas still hatin'\n",
      "So much hate, I need a AK (AK)\n",
      "Now we out in Paris, yeah, I'm Perrierin'\n",
      "White girls politickin': that's that Sarah Palin\n",
      "Get–get–get–get–get–gettin' hot, Californicatin'\n",
      "I give her that D, 'cause that's where I was born and raised in\n",
      "\n",
      "[Chorus: YB, Big Sean & Fuzzy Jones]\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve)\n",
      "With your girl, she tryna jerk me (Swerve, believe)\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve)\n",
      "With your girl, she tryna jerk me (Swerve)\n",
      "\n",
      "[Post-Chorus: Fuzzy Jones & Big Sean]\n",
      "Well, it is a weepin' and a moanin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a weepin' and a mournin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a—when it comes to my sound (Swerve)\n",
      "Which is the champion sound (Swerve)\n",
      "Believe, believe (Swerve)\n",
      "Believe, believe (Swerve)\n",
      "\n",
      "[Verse 2: Pusha T]\n",
      "Yuugh! It's prime time, my top back, this pimp game, ho\n",
      "I'm red leather, this cocaine, I'm Rick James, ho\n",
      "I'm bill-droppin', Ms. Pac-Man, this pill-poppin' ass ho\n",
      "I'm poppin' too, these blue dolphins need two coffins\n",
      "All she want is some heel money\n",
      "All she need is some bill money\n",
      "He take his time, he counts it out\n",
      "I weighs it up, that's real money\n",
      "Check the neck, check the wrist\n",
      "Them heads turnin': that's exorcist\n",
      "My Audemars like Mardi Gras\n",
      "That's Swiss time, and that's excellence\n",
      "Two-door preference\n",
      "Roof gone, George Jefferson\n",
      "That white frost on that pound cake\n",
      "So your Duncan Hines is irrelevant, woo\n",
      "Lambo Murciélago\n",
      "She go wherever I go\n",
      "Wherever we go, we do it pronto, it's like—\n",
      "\n",
      "[Chorus: YB, Big Sean & Fuzzy Jones]\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve)\n",
      "With your girl, she tryna jerk me (Swerve, believe)\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve)\n",
      "With your girl, she tryna jerk me (Swerve)\n",
      "\n",
      "[Post-Chorus: Fuzzy Jones & Big Sean]\n",
      "Well, it is a weepin' and a moanin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a weepin' and a mournin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a—when it comes to my sound (Swerve)\n",
      "Which is the champion sound (Swerve)\n",
      "Believe, believe (Swerve)\n",
      "Believe (Swerve)\n",
      "\n",
      "[Bridge: Fuzzy Jones]\n",
      "Well, it is a weepin' and a moanin'\n",
      "And a gnashin' of teeth in the dancehall\n",
      "And who no have teeth gwan rub pon dem gums\n",
      "Cuh when time it comes to my sound\n",
      "Which is the champion sound\n",
      "The bugle has blown fi many times\n",
      "And it still have one more time left\n",
      "Cuh the amount of stripe weh deh pon our shoulder\n",
      "\n",
      "[Verse 3: Kanye West & 2 Chainz]\n",
      "Let the suicide doors up\n",
      "I threw suicides on the tour bus\n",
      "I threw suicides on the private jet\n",
      "You know what that mean, I'm fly to death\n",
      "I step in Def Jam buildin' like I'm the shit\n",
      "Tell 'em gimme fifty million or I'ma quit\n",
      "Most rappers' taste level ain't at my waist level\n",
      "Turn up the bass 'til it's up-in-yo-face level\n",
      "Don't do no press but I get the most press kit\n",
      "Plus, yo, my bitch make your bitch look like Precious\n",
      "Somethin' 'bout Mary, she gone off that molly\n",
      "Now the whole party is melting like Dalí\n",
      "Now everybody is movin' they body\n",
      "Don't sell me apartment, I'll move in the lobby (Yah)\n",
      "Niggas is loiterin' just to feel important\n",
      "You gon' see lawyers and niggas in Jordans (2 Chainz)\n",
      "\n",
      "[Verse 4: 2 Chainz & Big Sean]\n",
      "Okay, now catch up to my campaign\n",
      "Coupe the color of mayonnaise\n",
      "I'm drunk and high at the same time\n",
      "Drinkin' champagne on the airplane (Tell 'em)\n",
      "Spit rounds like a gun range (Baow)\n",
      "Beat it up like Rampage\n",
      "Hundred bands, cut your girl\n",
      "Now your girl need a Band-Aid (Damn)\n",
      "Grade A, A1\n",
      "Chain the color of Akon\n",
      "Black diamonds, backpack rhymin'\n",
      "Co-signed by Louis Vuitton (Yep)\n",
      "Horsepower, horsepower\n",
      "All this Polo on, I got horsepower\n",
      "Pound of this cost four thousand\n",
      "Now I make it rain, she want more showers\n",
      "Rain (Rain) pourin' (Pourin')\n",
      "All my cars is foreign (Foreign)\n",
      "All my broads is foreign (Foreign)\n",
      "Money tall like Jordan\n",
      "\n",
      "[Chorus: YB, Big Sean & Fuzzy Jones]\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve)\n",
      "With your girl, she tryna jerk me (Swerve)\n",
      "O-o-o-o-o-okay, Lamborghini Mercy (Swerve)\n",
      "Your chick, she so thirsty (Swerve)\n",
      "I-I-I-I-I'm in that two-seat Lambo (Swerve, believe)\n",
      "With your girl, she tryna jerk me (Swerve, believe)\n",
      "\n",
      "[Post-Chorus: Fuzzy Jones & Big Sean]\n",
      "Well, it is a weepin' and a moanin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a weepin' and a mournin' (Swerve)\n",
      "And a gnashin' of teeth (Swerve)\n",
      "It is a—when it comes to my sound (Swerve)\n",
      "Which is the champion sound (Swerve)\n",
      "Believe, believe (Swerve)\n",
      "Believe, believe (Swerve)\n",
      "Well, it is a weepin' and a moanin' and a gnashin' of teeth\n",
      "It is a weepin' and a mournin' and a gnashin' of teeth\n",
      "It is a—when it comes to my sound which is the champion sound\n",
      "Believe, believe, believe, believe\n",
      "<end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.songs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(42069)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nnet(\n",
       "  (lstm_layer): LSTM(132, 300, num_layers=3, dropout=0.25)\n",
       "  (fc): Linear(in_features=300, out_features=132, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(dataset.char_dict)\n",
    "hidden_dim = 300\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "\n",
    "model = Nnet(input_dim, hidden_dim, n_layers, dropout=0.25)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss criteria are defined in the torch.nn package\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=0.00001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 1/200... Training Loss: 3.275833... Validation Loss: 3.269704\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (inf --> 3.269704).  Saving model ...\n",
      "Epoch: 2/200... Training Loss: 3.271451... Validation Loss: 3.265373\n",
      "Validation loss decreased (3.269704 --> 3.265373).  Saving model ...\n",
      "Epoch: 3/200... Training Loss: 2.910989... Validation Loss: 2.900086\n",
      "Validation loss decreased (3.265373 --> 2.900086).  Saving model ...\n",
      "Epoch: 4/200... Training Loss: 2.606119... Validation Loss: 2.592266\n",
      "Validation loss decreased (2.900086 --> 2.592266).  Saving model ...\n",
      "Epoch: 5/200... Training Loss: 2.397590... Validation Loss: 2.380610\n",
      "Validation loss decreased (2.592266 --> 2.380610).  Saving model ...\n",
      "Epoch: 6/200... Training Loss: 2.258909... Validation Loss: 2.243620\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (2.380610 --> 2.243620).  Saving model ...\n",
      "Epoch: 7/200... Training Loss: 2.160080... Validation Loss: 2.149972\n",
      "Validation loss decreased (2.243620 --> 2.149972).  Saving model ...\n",
      "Epoch: 8/200... Training Loss: 2.077958... Validation Loss: 2.068495\n",
      "Validation loss decreased (2.149972 --> 2.068495).  Saving model ...\n",
      "Epoch: 9/200... Training Loss: 2.020578... Validation Loss: 2.017026\n",
      "Validation loss decreased (2.068495 --> 2.017026).  Saving model ...\n",
      "Epoch: 10/200... Training Loss: 1.947178... Validation Loss: 1.945131\n",
      "Validation loss decreased (2.017026 --> 1.945131).  Saving model ...\n",
      "Epoch: 11/200... Training Loss: 1.897352... Validation Loss: 1.896961\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.945131 --> 1.896961).  Saving model ...\n",
      "Epoch: 12/200... Training Loss: 1.858232... Validation Loss: 1.860096\n",
      "Validation loss decreased (1.896961 --> 1.860096).  Saving model ...\n",
      "Epoch: 13/200... Training Loss: 1.824209... Validation Loss: 1.829166\n",
      "Validation loss decreased (1.860096 --> 1.829166).  Saving model ...\n",
      "Epoch: 14/200... Training Loss: 1.797709... Validation Loss: 1.807020\n",
      "Validation loss decreased (1.829166 --> 1.807020).  Saving model ...\n",
      "Epoch: 15/200... Training Loss: 1.762070... Validation Loss: 1.774066\n",
      "Validation loss decreased (1.807020 --> 1.774066).  Saving model ...\n",
      "Epoch: 16/200... Training Loss: 1.739190... Validation Loss: 1.753977\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.774066 --> 1.753977).  Saving model ...\n",
      "Epoch: 17/200... Training Loss: 1.711994... Validation Loss: 1.728725\n",
      "Validation loss decreased (1.753977 --> 1.728725).  Saving model ...\n",
      "Epoch: 18/200... Training Loss: 1.684964... Validation Loss: 1.704868\n",
      "Validation loss decreased (1.728725 --> 1.704868).  Saving model ...\n",
      "Epoch: 19/200... Training Loss: 1.662313... Validation Loss: 1.686304\n",
      "Validation loss decreased (1.704868 --> 1.686304).  Saving model ...\n",
      "Epoch: 20/200... Training Loss: 1.645583... Validation Loss: 1.670219\n",
      "Validation loss decreased (1.686304 --> 1.670219).  Saving model ...\n",
      "Epoch: 21/200... Training Loss: 1.633449... Validation Loss: 1.660594\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.670219 --> 1.660594).  Saving model ...\n",
      "Epoch: 22/200... Training Loss: 1.610321... Validation Loss: 1.641308\n",
      "Validation loss decreased (1.660594 --> 1.641308).  Saving model ...\n",
      "Epoch: 23/200... Training Loss: 1.597666... Validation Loss: 1.631277\n",
      "Validation loss decreased (1.641308 --> 1.631277).  Saving model ...\n",
      "Epoch: 24/200... Training Loss: 1.571091... Validation Loss: 1.607031\n",
      "Validation loss decreased (1.631277 --> 1.607031).  Saving model ...\n",
      "Epoch: 25/200... Training Loss: 1.559582... Validation Loss: 1.599743\n",
      "Validation loss decreased (1.607031 --> 1.599743).  Saving model ...\n",
      "Epoch: 26/200... Training Loss: 1.543160... Validation Loss: 1.586215\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.599743 --> 1.586215).  Saving model ...\n",
      "Epoch: 27/200... Training Loss: 1.534151... Validation Loss: 1.580251\n",
      "Validation loss decreased (1.586215 --> 1.580251).  Saving model ...\n",
      "Epoch: 28/200... Training Loss: 1.523158... Validation Loss: 1.571227\n",
      "Validation loss decreased (1.580251 --> 1.571227).  Saving model ...\n",
      "Epoch: 29/200... Training Loss: 1.508775... Validation Loss: 1.560468\n",
      "Validation loss decreased (1.571227 --> 1.560468).  Saving model ...\n",
      "Epoch: 30/200... Training Loss: 1.496520... Validation Loss: 1.550299\n",
      "Validation loss decreased (1.560468 --> 1.550299).  Saving model ...\n",
      "Epoch: 31/200... Training Loss: 1.487353... Validation Loss: 1.542440\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.550299 --> 1.542440).  Saving model ...\n",
      "Epoch: 32/200... Training Loss: 1.479069... Validation Loss: 1.535242\n",
      "Validation loss decreased (1.542440 --> 1.535242).  Saving model ...\n",
      "Epoch: 33/200... Training Loss: 1.463085... Validation Loss: 1.523672\n",
      "Validation loss decreased (1.535242 --> 1.523672).  Saving model ...\n",
      "Epoch: 34/200... Training Loss: 1.455429... Validation Loss: 1.520494\n",
      "Validation loss decreased (1.523672 --> 1.520494).  Saving model ...\n",
      "Epoch: 35/200... Training Loss: 1.441181... Validation Loss: 1.506620\n",
      "Validation loss decreased (1.520494 --> 1.506620).  Saving model ...\n",
      "Epoch: 36/200... Training Loss: 1.436934... Validation Loss: 1.505485\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.506620 --> 1.505485).  Saving model ...\n",
      "Epoch: 37/200... Training Loss: 1.423450... Validation Loss: 1.496288\n",
      "Validation loss decreased (1.505485 --> 1.496288).  Saving model ...\n",
      "Epoch: 38/200... Training Loss: 1.413830... Validation Loss: 1.490346\n",
      "Validation loss decreased (1.496288 --> 1.490346).  Saving model ...\n",
      "Epoch: 39/200... Training Loss: 1.407867... Validation Loss: 1.484461\n",
      "Validation loss decreased (1.490346 --> 1.484461).  Saving model ...\n",
      "Epoch: 40/200... Training Loss: 1.401422... Validation Loss: 1.482270\n",
      "Validation loss decreased (1.484461 --> 1.482270).  Saving model ...\n",
      "Epoch: 41/200... Training Loss: 1.390913... Validation Loss: 1.475043\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.482270 --> 1.475043).  Saving model ...\n",
      "Epoch: 42/200... Training Loss: 1.383228... Validation Loss: 1.470853\n",
      "Validation loss decreased (1.475043 --> 1.470853).  Saving model ...\n",
      "Epoch: 43/200... Training Loss: 1.372927... Validation Loss: 1.464042\n",
      "Validation loss decreased (1.470853 --> 1.464042).  Saving model ...\n",
      "Epoch: 44/200... Training Loss: 1.373348... Validation Loss: 1.463732\n",
      "Validation loss decreased (1.464042 --> 1.463732).  Saving model ...\n",
      "Epoch: 45/200... Training Loss: 1.361554... Validation Loss: 1.456949\n",
      "Validation loss decreased (1.463732 --> 1.456949).  Saving model ...\n",
      "Epoch: 46/200... Training Loss: 1.350474... Validation Loss: 1.453320\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.456949 --> 1.453320).  Saving model ...\n",
      "Epoch: 47/200... Training Loss: 1.350014... Validation Loss: 1.454107\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 48/200... Training Loss: 1.344004... Validation Loss: 1.451189\n",
      "Validation loss decreased (1.453320 --> 1.451189).  Saving model ...\n",
      "Epoch: 49/200... Training Loss: 1.329951... Validation Loss: 1.436709\n",
      "Validation loss decreased (1.451189 --> 1.436709).  Saving model ...\n",
      "Epoch: 50/200... Training Loss: 1.324745... Validation Loss: 1.437880\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 51/200... Training Loss: 1.316612... Validation Loss: 1.433136\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.436709 --> 1.433136).  Saving model ...\n",
      "Epoch: 52/200... Training Loss: 1.314137... Validation Loss: 1.429703\n",
      "Validation loss decreased (1.433136 --> 1.429703).  Saving model ...\n",
      "Epoch: 53/200... Training Loss: 1.302801... Validation Loss: 1.425609\n",
      "Validation loss decreased (1.429703 --> 1.425609).  Saving model ...\n",
      "Epoch: 54/200... Training Loss: 1.296909... Validation Loss: 1.421942\n",
      "Validation loss decreased (1.425609 --> 1.421942).  Saving model ...\n",
      "Epoch: 55/200... Training Loss: 1.292218... Validation Loss: 1.423522\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 56/200... Training Loss: 1.290442... Validation Loss: 1.421217\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.421942 --> 1.421217).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/200... Training Loss: 1.280218... Validation Loss: 1.415784\n",
      "Validation loss decreased (1.421217 --> 1.415784).  Saving model ...\n",
      "Epoch: 58/200... Training Loss: 1.275131... Validation Loss: 1.413802\n",
      "Validation loss decreased (1.415784 --> 1.413802).  Saving model ...\n",
      "Epoch: 59/200... Training Loss: 1.271073... Validation Loss: 1.409077\n",
      "Validation loss decreased (1.413802 --> 1.409077).  Saving model ...\n",
      "Epoch: 60/200... Training Loss: 1.260953... Validation Loss: 1.407854\n",
      "Validation loss decreased (1.409077 --> 1.407854).  Saving model ...\n",
      "Epoch: 61/200... Training Loss: 1.255070... Validation Loss: 1.406794\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.407854 --> 1.406794).  Saving model ...\n",
      "Epoch: 62/200... Training Loss: 1.246606... Validation Loss: 1.398685\n",
      "Validation loss decreased (1.406794 --> 1.398685).  Saving model ...\n",
      "Epoch: 63/200... Training Loss: 1.243555... Validation Loss: 1.399462\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 64/200... Training Loss: 1.236178... Validation Loss: 1.397202\n",
      "Validation loss decreased (1.398685 --> 1.397202).  Saving model ...\n",
      "Epoch: 65/200... Training Loss: 1.231445... Validation Loss: 1.393287\n",
      "Validation loss decreased (1.397202 --> 1.393287).  Saving model ...\n",
      "Epoch: 66/200... Training Loss: 1.226412... Validation Loss: 1.393277\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.393287 --> 1.393277).  Saving model ...\n",
      "Epoch: 67/200... Training Loss: 1.218554... Validation Loss: 1.388888\n",
      "Validation loss decreased (1.393277 --> 1.388888).  Saving model ...\n",
      "Epoch: 68/200... Training Loss: 1.217901... Validation Loss: 1.391353\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 69/200... Training Loss: 1.209588... Validation Loss: 1.389326\n",
      "Early stopping is enabled. Training will stop in 13 epochs if validation loss does not go down.\n",
      "Epoch: 70/200... Training Loss: 1.203757... Validation Loss: 1.385896\n",
      "Validation loss decreased (1.388888 --> 1.385896).  Saving model ...\n",
      "Epoch: 71/200... Training Loss: 1.197663... Validation Loss: 1.382191\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.385896 --> 1.382191).  Saving model ...\n",
      "Epoch: 72/200... Training Loss: 1.196102... Validation Loss: 1.384605\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 73/200... Training Loss: 1.191479... Validation Loss: 1.381036\n",
      "Validation loss decreased (1.382191 --> 1.381036).  Saving model ...\n",
      "Epoch: 74/200... Training Loss: 1.186250... Validation Loss: 1.381488\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 75/200... Training Loss: 1.177162... Validation Loss: 1.376414\n",
      "Validation loss decreased (1.381036 --> 1.376414).  Saving model ...\n",
      "Epoch: 76/200... Training Loss: 1.172997... Validation Loss: 1.369878\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.376414 --> 1.369878).  Saving model ...\n",
      "Epoch: 77/200... Training Loss: 1.167665... Validation Loss: 1.374814\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 78/200... Training Loss: 1.165709... Validation Loss: 1.374410\n",
      "Early stopping is enabled. Training will stop in 13 epochs if validation loss does not go down.\n",
      "Epoch: 79/200... Training Loss: 1.160242... Validation Loss: 1.370366\n",
      "Early stopping is enabled. Training will stop in 12 epochs if validation loss does not go down.\n",
      "Epoch: 80/200... Training Loss: 1.148838... Validation Loss: 1.366892\n",
      "Validation loss decreased (1.369878 --> 1.366892).  Saving model ...\n",
      "Epoch: 81/200... Training Loss: 1.145275... Validation Loss: 1.365248\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.366892 --> 1.365248).  Saving model ...\n",
      "Epoch: 82/200... Training Loss: 1.137160... Validation Loss: 1.364772\n",
      "Validation loss decreased (1.365248 --> 1.364772).  Saving model ...\n",
      "Epoch: 83/200... Training Loss: 1.137617... Validation Loss: 1.361442\n",
      "Validation loss decreased (1.364772 --> 1.361442).  Saving model ...\n",
      "Epoch: 84/200... Training Loss: 1.130571... Validation Loss: 1.363529\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 85/200... Training Loss: 1.126222... Validation Loss: 1.367005\n",
      "Early stopping is enabled. Training will stop in 13 epochs if validation loss does not go down.\n",
      "Epoch: 86/200... Training Loss: 1.122332... Validation Loss: 1.357713\n",
      "Creating training checkpoint...\n",
      "Validation loss decreased (1.361442 --> 1.357713).  Saving model ...\n",
      "Epoch: 87/200... Training Loss: 1.117072... Validation Loss: 1.359424\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n",
      "Epoch: 88/200... Training Loss: 1.123178... Validation Loss: 1.360324\n",
      "Early stopping is enabled. Training will stop in 13 epochs if validation loss does not go down.\n",
      "Epoch: 89/200... Training Loss: 1.106077... Validation Loss: 1.358298\n",
      "Early stopping is enabled. Training will stop in 12 epochs if validation loss does not go down.\n",
      "Epoch: 90/200... Training Loss: 1.102501... Validation Loss: 1.356767\n",
      "Validation loss decreased (1.357713 --> 1.356767).  Saving model ...\n",
      "Epoch: 91/200... Training Loss: 1.095705... Validation Loss: 1.357898\n",
      "Creating training checkpoint...\n",
      "Early stopping is enabled. Training will stop in 14 epochs if validation loss does not go down.\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "clip = 50\n",
    "chunk_len = 300\n",
    "early_stop = True\n",
    "early_stop_thresh = 15\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "val_losses_epoch = []\n",
    "train_losses_epoch = []\n",
    "\n",
    "def get_labels(inputs):\n",
    "    labels = torch.zeros(inputs.shape[0])\n",
    "\n",
    "    for i,oneHot in enumerate(inputs[1:]):\n",
    "        idx = oneHot.argmax().item()\n",
    "        labels[i] = idx\n",
    "        \n",
    "    return labels\n",
    "\n",
    "\n",
    "index_to_char = {v: k for k, v in dataset.char_dict.items()}\n",
    "\n",
    "model.train()\n",
    "print(\"Training...\")\n",
    "for epoch in range(epochs):\n",
    "    # training process\n",
    "    for sample_idx, song in enumerate(train_loader):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs, h)\n",
    "            loss = criterion(output, labels)\n",
    "            h = tuple([e.detach() for e in h])\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "    # validation process\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    #print(\"Evaluating on validation set...\")\n",
    "    for sample_idx, song in enumerate(val_loader):\n",
    "#         if sample_idx % 100 == 0:\n",
    "#             print(\"Evaluating on song %d (%d/%d)\"  % (sample_idx, sample_idx, len(val_loader)))\n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        \n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "    # evaluate on the training set to get training loss metrics\n",
    "    train_losses = []\n",
    "    #print(\"Evaluting on training set...\")\n",
    "    for sample_idx, song in enumerate(train_loader):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        song_chunks = chunkstring(song[0], chunk_len)\n",
    "        for chunk in song_chunks:\n",
    "            inputs = encode_chunk(chunk, dataset.char_dict)\n",
    "            labels = get_labels(inputs)\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            output, h = model(inputs, h)\n",
    "            train_loss = criterion(output, labels)\n",
    "            train_losses.append(train_loss.item())\n",
    "    \n",
    "    \n",
    "    train_losses_epoch.append(np.mean(train_losses))\n",
    "    val_losses_epoch.append(np.mean(val_losses))\n",
    "\n",
    "    model.train()\n",
    "    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "          \"Training Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "          \"Validation Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Creating training checkpoint...')\n",
    "        # save training progress every five epochs\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, 'kanye_state_ckpt.pth')\n",
    "    \n",
    "    if val_losses_epoch[epoch] <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './kanye_state_dict.pth')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "        valid_loss_min = np.mean(val_losses)\n",
    "        \n",
    "        if early_stop:\n",
    "            early_stop_strikes = 0\n",
    "    \n",
    "    else:\n",
    "        if early_stop:\n",
    "            early_stop_strikes += 1\n",
    "            if early_stop_strikes >= early_stop_thresh:\n",
    "                print(\"Early stopping is enabled. Training will now stop.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Early stopping is enabled. Training will stop in %d epochs if validation loss does not go down.\"\n",
    "                    % (early_stop_thresh-early_stop_strikes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(train_losses_epoch)\n",
    "plt.plot(val_losses_epoch)\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss (averaged over each chunk)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from torch.distributions.categorical import Categorical\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "model = Nnet(input_dim, hidden_dim, n_layers, dropout=0.25)\n",
    "model.load_state_dict(torch.load('kanye_state_dict.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_next_char(model, char_dict, hidden, input_token='<start>', sampling_scheme='softmax', temperature=0.7):\n",
    "    '''Gets the next character given a chunk input_token\n",
    "    :param: a pytorch neural network to be passed\n",
    "    :char_dict: a dictionary mapping alphanumeric characters of type str to integers\n",
    "    :input_token: the string to begin on for the LSTM\n",
    "    '''\n",
    "    inputs = encode_chunk(input_token, char_dict)\n",
    "    h = tuple([e.data for e in hidden])\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    if sampling_scheme == 'softmax':\n",
    "        prediction_vector = softmax(output / temperature, dim=1)\n",
    "        output_dist = Categorical(probs=prediction_vector)\n",
    "        next_char = output_dist.sample()[-1]\n",
    "    else:\n",
    "        # argmax sampling scheme\n",
    "        prediction_vector = softmax(output, dim=1)\n",
    "        next_char = prediction_vector.argmax(dim=1)[-1]\n",
    "    \n",
    "    return next_char, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_file_size = 3000 # characters\n",
    "model_name = 'KanyeWest_LSTM'\n",
    "\n",
    "next_chunk = '<start>\\nTitle:'\n",
    "\n",
    "out = deepcopy(next_chunk)\n",
    "    \n",
    "index_to_char = {v: k for k, v in dataset.char_dict.items()}\n",
    "\n",
    "max_chunk_size = 500\n",
    "#try passing in the token with 0'd hidden units at the beginning then take the hidden unit output\n",
    "#from that and pass that back in with the token again and go on from there\n",
    "hidden = model.init_hidden(1)\n",
    "for progress,i in enumerate(range(generated_file_size)):\n",
    "    if progress % 500 == 0:\n",
    "        print(\"Generation progress: %d/%d characters\" % (progress, generated_file_size))\n",
    "    if progress == 0 or next_chunk[-len('<end>'):] == '<end>':\n",
    "        if progress > 0:\n",
    "            next_chunk = '<start>\\n'\n",
    "            out += '\\n<start>\\n'\n",
    "        #hidden = model.init_hidden(1)\n",
    "    \n",
    "    (next_char, h) = get_next_char(model, dataset.char_dict, hidden, input_token=next_chunk)\n",
    "    char = index_to_char[next_char.item()]\n",
    "    \n",
    "    if len(next_chunk) < max_chunk_size:\n",
    "        next_chunk += char\n",
    "    else:\n",
    "        next_chunk = next_chunk[1:] + char\n",
    "    out += char\n",
    "    \n",
    "with open(\"generated_songs/generated_songs_\" + model_name + \".txt\", \"w\") as text_file:\n",
    "    print(out, file=text_file)\n",
    "    \n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
